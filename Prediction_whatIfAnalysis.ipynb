{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khushbu\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# !pip install urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os \n",
    "import time\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import h2o\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.svm import SVR\n",
    "from itertools import chain, combinations\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseUrl='https://freddiemac.embs.com/FLoan/'\n",
    "postUrl='Data/download.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCredentialData(user, passwd):\n",
    "    creds={'username': user,'password': passwd}\n",
    "    return creds\n",
    "\n",
    "def getFilesFromFreddieMac(cred):\n",
    "    ## We are using inside WITH BLock so that session is closed ASAP with BLock is exited \n",
    "    with requests.Session() as s:\n",
    "        ## Step 1 routing to auth.php Site with the proper crentials \n",
    "        urlOne = s.post(baseUrl+\"secure/auth.php\", data=cred) \n",
    "        if \"Please log in\" in urlOne.text:\n",
    "        ## IF CREDENTIALS are not valid Throw Alert \n",
    "            print(\"Alert: Invalid Credentials, Please try again or sign up on below site \\n https://freddiemac.embs.com/FLoan/Bin/loginrequest.php\")\n",
    "        else:\n",
    "            print(\"Step1: Logged in\")\n",
    "        ## Sterp 2 Preparing the data for to Accept terms and Conditions \n",
    "            pay2={'accept': 'Yes','acceptSubmit':'Continue','action':'acceptTandC'}\n",
    "            finalUrl=s.post(baseUrl +\"Data/download.php\",pay2)\n",
    "            if \"Loan-Level Dataset\" in finalUrl.text:\n",
    "                      print(\"Step2 : Terms and Conditions Accepted\")\n",
    "                      soup = BeautifulSoup(finalUrl.content, \"html.parser\")   \n",
    "                      links_list = soup.findAll('a')\n",
    "                      print(\"Step3: Filtered the Sample Files with Condition== 2007/20008/2009/1999/2013\")\n",
    "                      print(\"Status::::::::::\")\n",
    "                      for ele in links_list:\n",
    "        ## Filtering the ZIp files >= 2005 \n",
    "                         if 'historical' in ele.get_text():\n",
    "                            if(ele.get_text()[-8:-4] == '2007' or ele.get_text()[-8:-4] == '2008' or ele.get_text()[-8:-4] == '2009' or ele.get_text()[-8:-4] == '2010' or ele.get_text()[-8:-4] == '2013' or ele.get_text()[-8:-4] == '2014'  or ele.get_text()[-8:-4] == '1999' or ele.get_text()[-8:-4] == '2000'):\n",
    "                                    print(ele.get_text()[-8:-4])\n",
    "                                    tempUrl = baseUrl+\"Data/\"+ele.get('href')                         \n",
    "                                    b =time.time()\n",
    "                                    downloadUrl=s.post(tempUrl) ## return type = Response\n",
    "                                    e=time.time()\n",
    "                                    print(tempUrl + \" took \"+ str(e-b)+\" sec\")\n",
    "                                    with ZipFile(BytesIO(downloadUrl.content)) as zfile:\n",
    "                                          zfile.extractall(os.path.join(fileDir, 'adsDataRepo/'+'Historical_data_'+ele.get_text()[-8:-4]+'/'))\n",
    "                                          print(\"File \"+ ele.get_text()+\" Downloaded\")\n",
    "    \n",
    "            else:\n",
    "                print(\"Alert: Please Check the rerouting action suffix\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessData(inputQuater,inputYear,inputQuaterTwo,inputYearTwo):\n",
    "    cleandataOne= \"\"\n",
    "    cleandataTwo= \"\"\n",
    "    print(\"pre-process data\")\n",
    "    if(os.path.exists(fileDir+'/adsDataRepo/')):\n",
    "        trainingDataFile = glob.glob(fileDir+'/adsDataRepo/'+'Historical_data_'+inputYear+'/historical_data1_'+inputQuater+inputYear+'.txt')\n",
    "        testingDataFile = glob.glob(fileDir+'/adsDataRepo/'+'Historical_data_'+inputYearTwo+'/historical_data1_'+inputQuaterTwo+inputYearTwo+'.txt')\n",
    "        headerNames = ['CreditScore','FirstPaymentDate','FirstTimeHomeBuyerFlag','MaturityDate','MSA','MIP','NumberOfUnits',\n",
    "                         'OccupancyStatus','OCLTV','DTI','OriginalUPB','OLTV','OriginalInterestRate','Channel','PrepaymentPenaltyFlag',\n",
    "                         'ProductType','PropertyState','PropertyType','PostalCode','LoanSequenceNumber','LoanPurpose',\n",
    "                         'OriginalLoanTerm','NumberOfBorrowers','SellerName','ServicerName','SuperConformingFlag']\n",
    "        with open(trainingDataFile[0]) as f:\n",
    "            dataf = pd.read_table(f, sep='|', low_memory=False, header=None,lineterminator='\\n', names= headerNames)\n",
    "            cleandataOne = originationDatacleaning(dataf)\n",
    "            cleandataOne.to_csv(\"Origination_Clean_\"+inputQuater+inputYear+\".csv\",index=False)\n",
    "            print(\"training data cleaned, CSV Created\")\n",
    "       \n",
    "        with open(testingDataFile[0]) as f:\n",
    "            dataf = pd.read_table(f, sep='|', low_memory=False, header=None,lineterminator='\\n', names= headerNames)\n",
    "            cleandataTwo = originationDatacleaning(dataf)\n",
    "            cleandataTwo.to_csv(\"Origination_Clean_\"+inputQuaterTwo+inputYearTwo+\".csv\",index=False)\n",
    "            print(\"testing data cleaned, CSV Created\")\n",
    "\n",
    "    return cleandataOne,cleandataTwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def originationDatacleaning(dataf):\n",
    "    dataf['CreditScore'].replace('   ',301,inplace=True)\n",
    "    dataf['CreditScore'].fillna(301,inplace=True)\n",
    "    dataf['FirstTimeHomeBuyerFlag'].fillna('X',inplace=True) \n",
    "    dataf['MSA'].replace('   ',0,inplace=True)\n",
    "    dataf['MSA'].fillna(0, inplace=True) \n",
    "    dataf['MIP'].replace('   ',0,inplace=True)\n",
    "    dataf['MIP'].fillna(0, inplace=True)\n",
    "    dataf['NumberOfUnits'].fillna(0,inplace=True)\n",
    "    dataf['OccupancyStatus'].fillna('X',inplace=True)\n",
    "    dataf['OCLTV'].replace('   ',0,inplace=True)\n",
    "    dataf['OCLTV'].fillna(0,inplace=True)\n",
    "    dataf['DTI'].replace('   ',0,inplace=True)\n",
    "    dataf['DTI'].fillna(0,inplace=True)\n",
    "    dataf['OriginalUPB'].replace('   ',0,inplace=True)\n",
    "    dataf['OriginalUPB'].fillna(0,inplace=True)\n",
    "    dataf['OLTV'].replace('   ',0,inplace=True)\n",
    "    dataf['OLTV'].fillna(0,inplace=True)\n",
    "    dataf['OriginalInterestRate'].fillna(0,inplace=True)\n",
    "    dataf['Channel'].fillna('X',inplace=True)\n",
    "    dataf['PrepaymentPenaltyFlag'].fillna('X',inplace=True)\n",
    "    dataf['ProductType'].fillna('XXXXX',inplace=True)\n",
    "    dataf['PropertyState'].fillna('XX',inplace=True)\n",
    "    dataf['PropertyType'].fillna('XX',inplace=True)\n",
    "    dataf['PostalCode'].fillna(0,inplace=True)\n",
    "    dataf['LoanSequenceNumber'].replace('', np.NaN).fillna(0,inplace=True)\n",
    "    dataf['LoanPurpose'].fillna('X',inplace=True)\n",
    "    dataf['OriginalLoanTerm'].replace('', np.NaN).fillna(0,inplace=True)\n",
    "    dataf['NumberOfBorrowers'].fillna('01',inplace=True)\n",
    "    dataf['SellerName'].fillna('X',inplace=True)\n",
    "    dataf['ServicerName'].fillna('X',inplace=True)\n",
    "    dataf['SuperConformingFlag'].fillna('X',inplace=True)\n",
    "    \n",
    "    #factorizing data \n",
    "    factorizeCategoricalColumn(dataf)\n",
    "    \n",
    "    #assingning datatype\n",
    "    dataf[['PropertyState','LoanSequenceNumber']]=dataf[['PropertyState','LoanSequenceNumber']].astype('str')\n",
    "    dataf[['FirstTimeHomeBuyerFlag','OccupancyStatus','Channel','PrepaymentPenaltyFlag','ProductType','PropertyType','CreditScore','LoanPurpose','SellerName','ServicerName','MSA','MIP','NumberOfUnits','DTI','OCLTV','OLTV','PostalCode','NumberOfBorrowers']]=dataf[['FirstTimeHomeBuyerFlag','OccupancyStatus','Channel','PrepaymentPenaltyFlag','ProductType','PropertyType','CreditScore','LoanPurpose','SellerName','ServicerName','MSA','MIP','NumberOfUnits','DTI','OCLTV','OLTV','PostalCode','NumberOfBorrowers']].astype('int64')\n",
    "    \n",
    "    #missinganalysis(dataf)\n",
    "    \n",
    "    return dataf\n",
    "    '''As we can see we have the below Null Values presnt in the Data for all the Years (Only varying the Counts )\n",
    "                       MSA           \n",
    "    FirstTimeHomeBuyerFlag           \n",
    "     PrepaymentPenaltyFlag          \n",
    "         NumberOfBorrowers \n",
    "    We can ignore''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorizeCategoricalColumn(cleanperfTrain):\n",
    "        print('_________________________________________________________')\n",
    "        print('Factorizing the Categorical Columns .....................')\n",
    "        print('_________________________________________________________')\n",
    "\n",
    "        cleanperfTrain['FirstTimeHomeBuyerFlag'] = pd.factorize(cleanperfTrain['FirstTimeHomeBuyerFlag'])[0]\n",
    "        cleanperfTrain['OccupancyStatus'] = pd.factorize(cleanperfTrain['OccupancyStatus'])[0]\n",
    "        cleanperfTrain['Channel'] = pd.factorize(cleanperfTrain['Channel'])[0]\n",
    "        cleanperfTrain['ProductType'] = pd.factorize(cleanperfTrain['ProductType'])[0]\n",
    "        cleanperfTrain['PropertyType'] = pd.factorize(cleanperfTrain['PropertyType'])[0]\n",
    "        cleanperfTrain['LoanPurpose'] = pd.factorize(cleanperfTrain['LoanPurpose'])[0]\n",
    "        cleanperfTrain['SellerName'] = pd.factorize(cleanperfTrain['SellerName'])[0]\n",
    "        cleanperfTrain['ServicerName'] = pd.factorize(cleanperfTrain['ServicerName'])[0]\n",
    "        cleanperfTrain['PrepaymentPenaltyFlag'] = pd.factorize(cleanperfTrain['PrepaymentPenaltyFlag'])[0]\n",
    "        \n",
    "        return cleanperfTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropColumns(file):\n",
    "    file.drop(\"FirstPaymentDate\",axis=1,inplace=True)\n",
    "    file.drop(\"MaturityDate\",axis=1,inplace=True)\n",
    "    file.drop(\"PostalCode\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMae(model_mae,y,x):\n",
    "    model= model_mae\n",
    "    pred=model.predict(x)\n",
    "    mae=mean_absolute_error(y,pred);\n",
    "    print(\"MAE:\"+str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomForestRegressionAlgorithm(datadfTraining,datadfTesting):\n",
    "    label=datadfTraining.OriginalInterestRate\n",
    "    datadfTraining.drop('OriginalInterestRate',axis=1,inplace=True)\n",
    "    features=datadfTraining\n",
    "    labelTesting=datadfTesting.OriginalInterestRate\n",
    "    datadfTesting.drop('OriginalInterestRate',axis=1,inplace=True)\n",
    "    featuresTesting=datadfTesting\n",
    "    print(\"Training Data\")\n",
    "    rForest=RandomForestRegressor(max_depth=8)\n",
    "    rForest.fit(features,label)\n",
    "    computeMae(rForest,label,features)\n",
    "    computeRMSE(rForest,label,features)\n",
    "    computeMape(rForest,label,features)\n",
    "    print(\"Testing Data\")\n",
    "    computeMae(rForest,labelTesting,featuresTesting)\n",
    "    computeRMSE(rForest,labelTesting,featuresTesting)\n",
    "    computeMape(rForest,labelTesting,featuresTesting)\n",
    "    plt.scatter(rForest.predict(features),rForest.predict(features)-label,c='r',s=40,alpha=0.5)\n",
    "    plt.scatter(rForest.predict(featuresTesting),rForest.predict(featuresTesting)-labelTesting,c=\"b\",s=40)\n",
    "    plt.hlines(y=0,xmin=2,xmax=10)\n",
    "    plt.title('Residual plot using training(blue) and test(green) data')\n",
    "    plt.ylabel('Residuals')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeRMSE(model_rmse,y,x):\n",
    "    model= model_rmse\n",
    "    pred=model.predict(x)\n",
    "    rmse=math.sqrt(mean_squared_error(y,pred))\n",
    "    print(\"RMSE:\"+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMape(model_mape,y,x):\n",
    "    model= model_mape\n",
    "    pred=model.predict(x)\n",
    "    mape=np.mean(np.abs((y - pred) / y)) * 100\n",
    "    print( \"MAPE:\"+str(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: Logged in\n",
      "Step2 : Terms and Conditions Accepted\n",
      "Step3: Filtered the Sample Files with Condition== 2007/20008/2009/1999/2013\n",
      "Status::::::::::\n",
      "1999\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q11999&s=238424377 took 40.9420001507 sec\n",
      "File historical_data1_Q11999.zip Downloaded\n",
      "1999\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q21999&s=174529382 took 37.9309999943 sec\n",
      "File historical_data1_Q21999.zip Downloaded\n",
      "1999\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q31999&s=91809458 took 13.1579999924 sec\n",
      "File historical_data1_Q31999.zip Downloaded\n",
      "1999\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q41999&s=63771806 took 12.9240000248 sec\n",
      "File historical_data1_Q41999.zip Downloaded\n",
      "2000\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q12000&s=40688241 took 8.50200009346 sec\n",
      "File historical_data1_Q12000.zip Downloaded\n",
      "2000\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q22000&s=52061227 took 8.1970000267 sec\n",
      "File historical_data1_Q22000.zip Downloaded\n",
      "2000\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q32000&s=57177702 took 10.9470000267 sec\n",
      "File historical_data1_Q32000.zip Downloaded\n",
      "2000\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q42000&s=63016330 took 10.8989999294 sec\n",
      "File historical_data1_Q42000.zip Downloaded\n",
      "2007\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q12007&s=185475454 took 44.3899998665 sec\n",
      "File historical_data1_Q12007.zip Downloaded\n",
      "2007\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q22007&s=208781855 took 50.0829999447 sec\n",
      "File historical_data1_Q22007.zip Downloaded\n",
      "2007\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q32007&s=150189907 took 35.8789999485 sec\n",
      "File historical_data1_Q32007.zip Downloaded\n",
      "2007\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q42007&s=157724564 took 24.7170000076 sec\n",
      "File historical_data1_Q42007.zip Downloaded\n",
      "2008\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q12008&s=214451213 took 52.3120000362 sec\n",
      "File historical_data1_Q12008.zip Downloaded\n",
      "2008\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q22008&s=179339633 took 31.5509998798 sec\n",
      "File historical_data1_Q22008.zip Downloaded\n",
      "2008\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q32008&s=86942476 took 13.8330001831 sec\n",
      "File historical_data1_Q32008.zip Downloaded\n",
      "2008\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q42008&s=88338222 took 15.0429999828 sec\n",
      "File historical_data1_Q42008.zip Downloaded\n",
      "2009\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q12009&s=304490915 took 55.0080001354 sec\n",
      "File historical_data1_Q12009.zip Downloaded\n",
      "2009\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q22009&s=371548112 took 67.0750000477 sec\n",
      "File historical_data1_Q22009.zip Downloaded\n",
      "2009\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q32009&s=204951166 took 46.7799999714 sec\n",
      "File historical_data1_Q32009.zip Downloaded\n",
      "2009\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q42009&s=184046529 took 32.2590000629 sec\n",
      "File historical_data1_Q42009.zip Downloaded\n",
      "2010\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q12010&s=132534365 took 25.5469999313 sec\n",
      "File historical_data1_Q12010.zip Downloaded\n",
      "2010\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    creds=createCredentialData(\"parekh.kh@husky.neu.edu\",\"UkQqsHbV\")\n",
    "    getFilesFromFreddieMac(creds)\n",
    "    print(\"2007 Analysis\")\n",
    "    files=preProcessData(\"Q1\",\"2007\",\"Q2\",\"2007\")\n",
    "    dropColumns(files[0])\n",
    "    dropColumns(files[1])\n",
    "    randomForestRegressionAlgorithm(files[0]._get_numeric_data(),files[1]._get_numeric_data())\n",
    "    filesOne=preProcessData(\"Q2\",\"2007\",\"Q3\",\"2007\")\n",
    "    dropColumns(filesOne[0])\n",
    "    dropColumns(filesOne[1])\n",
    "    randomForestRegressionAlgorithm(filesOne[0]._get_numeric_data(),filesOne[1]._get_numeric_data())\n",
    "    filesTwo=preProcessData(\"Q3\",\"2007\",\"Q4\",\"2007\")\n",
    "    dropColumns(filesTwo[0])\n",
    "    dropColumns(filesTwo[1])\n",
    "    randomForestRegressionAlgorithm(filesTwo[0]._get_numeric_data(),filesTwo[1]._get_numeric_data())\n",
    "    filesThree=preProcessData(\"Q4\",\"2007\",\"Q1\",\"2007\")\n",
    "    dropColumns(filesThree[0])\n",
    "    dropColumns(filesThree[1])\n",
    "    randomForestRegressionAlgorithm(filesThree[0]._get_numeric_data(),filesThree[1]._get_numeric_data())\n",
    "    \n",
    "    print(\"2009 Analysis\")\n",
    "    filesFour=preProcessData(\"Q1\",\"2009\",\"Q2\",\"2009\")\n",
    "    dropColumns(filesFour[0])\n",
    "    dropColumns(filesFour[1])\n",
    "    randomForestRegressionAlgorithm(filesFour[0]._get_numeric_data(),filesFour[1]._get_numeric_data())\n",
    "    filesFive=preProcessData(\"Q2\",\"2009\",\"Q1\",\"2010\")\n",
    "    dropColumns(filesFive[0])\n",
    "    dropColumns(filesFive[1])\n",
    "    randomForestRegressionAlgorithm(filesFive[0]._get_numeric_data(),filesFive[1]._get_numeric_data())\n",
    "    \n",
    "    print(\"1999 Analysis\")\n",
    "    filesSix=preProcessData(\"Q1\",\"1999\",\"Q2\",\"1999\")\n",
    "    dropColumns(filesSix[0])\n",
    "    dropColumns(filesSix[1])\n",
    "    randomForestRegressionAlgorithm(filesSix[0]._get_numeric_data(),filesSix[1]._get_numeric_data())\n",
    "    filesSeven=preProcessData(\"Q2\",\"1999\",\"Q3\",\"1999\")\n",
    "    dropColumns(filesSeven[0])\n",
    "    dropColumns(filesSeven[1])\n",
    "    randomForestRegressionAlgorithm(filesSeven[0]._get_numeric_data(),filesSeven[1]._get_numeric_data())\n",
    "    filesEight=preProcessData(\"Q3\",\"1999\",\"Q4\",\"1999\")\n",
    "    dropColumns(filesEight[0])\n",
    "    dropColumns(filesEight[1])\n",
    "    randomForestRegressionAlgorithm(filesEight[0]._get_numeric_data(),filesEight[1]._get_numeric_data())\n",
    "    filesNine=preProcessData(\"Q4\",\"1999\",\"Q1\",\"2000\")\n",
    "    dropColumns(filesNine[0])\n",
    "    dropColumns(filesNine[1])\n",
    "    randomForestRegressionAlgorithm(filesNine[0]._get_numeric_data(),filesNine[1]._get_numeric_data())\n",
    "    \n",
    "    print(\"2013 Analysis\")\n",
    "    filesTen=preProcessData(\"Q1\",\"2013\",\"Q2\",\"2013\")\n",
    "    dropColumns(filesTen[0])\n",
    "    dropColumns(filesTen[1])\n",
    "    randomForestRegressionAlgorithm(filesTen[0]._get_numeric_data(),filesTen[1]._get_numeric_data())\n",
    "    filesEleven=preProcessData(\"Q2\",\"2013\",\"Q3\",\"2013\")\n",
    "    dropColumns(filesEleven[0])\n",
    "    dropColumns(filesEleven[1])\n",
    "    randomForestRegressionAlgorithm(filesEleven[0]._get_numeric_data(),filesEleven[1]._get_numeric_data())\n",
    "    filesTwelve=preProcessData(\"Q3\",\"2013\",\"Q4\",\"2013\")\n",
    "    dropColumns(filesTwelve[0])\n",
    "    dropColumns(filesTwelve[1])\n",
    "    randomForestRegressionAlgorithm(filesTwelve[0]._get_numeric_data(),filesTwelve[1]._get_numeric_data())\n",
    "    filesThirteen=preProcessData(\"Q4\",\"2013\",\"Q1\",\"2014\")\n",
    "    dropColumns(filesThirteen[0])\n",
    "    dropColumns(filesThirteen[1])\n",
    "    randomForestRegressionAlgorithm(filesThirteen[0]._get_numeric_data(),filesThirteen[1]._get_numeric_data())\n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
