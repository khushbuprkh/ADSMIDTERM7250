{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of the Packages, Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# !pip install urllib\n",
    "# !pip install pandas\n",
    "# !pip install -U scikit-learn\n",
    "# !sudo pip install Pillow\n",
    "# !sudo pip install fabulous\n",
    "# !sudo pip install Pillow\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os \n",
    "import time\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import neural_network\n",
    "from pybrain.utilities import percentError\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules import SoftmaxLayer\n",
    "from pybrain.datasets.classification import ClassificationDataSet\n",
    "from pybrain.tools.validation import Validator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Making Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createCredentialData(user, passwd):\n",
    "    print('_________________________________________________________')\n",
    "    print('Creating Credentials ....')\n",
    "    creds={'username': user,'password': passwd}\n",
    "    return creds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Getting Files from the Freddie Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getFilesFromFreddieMac(cred,quater,year,quaterTwo,yearTwo,baseUrl,postUrl,fileDir):\n",
    "    print('_________________________________________________________')\n",
    "    print('GettingFile from Freddie Mac ......')\n",
    "    c=cred\n",
    "    ## We are using inside WITH BLock so that session is closed ASAP with BLock is exited \n",
    "    with requests.Session() as s:\n",
    "        ## Step 1 routing to auth.php Site with the proper crentials \n",
    "        urlOne = s.post(baseUrl+\"secure/auth.php\", data=cred) \n",
    "        if \"Please log in\" in urlOne.text:\n",
    "        ## IF CREDENTIALS are not valid Throw Alert \n",
    "            print(\"Alert: Invalid Credentials, Please try again or sign up on below site \\n https://freddiemac.embs.com/FLoan/Bin/loginrequest.php\")\n",
    "        else:\n",
    "            print(\"Step1: Logged in\")\n",
    "        ## Sterp 2 Preparing the data for to Accept terms and Conditions \n",
    "            pay2={'accept': 'Yes','acceptSubmit':'Continue','action':'acceptTandC'}\n",
    "            finalUrl=s.post(baseUrl +\"Data/download.php\",pay2)\n",
    "            if \"Loan-Level Dataset\" in finalUrl.text:\n",
    "                      print(\"Step2 : Terms and Conditions Accepted\")\n",
    "                      soup = BeautifulSoup(finalUrl.content, \"html.parser\")   \n",
    "                      links_list = soup.findAll('a')\n",
    "                      print(\"Step3: Filtered the Sample Files with Condition =\" + year)\n",
    "                      print(\"Status::::::::::\")\n",
    "                      for ele in links_list:\n",
    "        ## Filtering the ZIp files = 2005 \n",
    "                         if 'historical' in ele.get_text():\n",
    "                          \n",
    "                            while(ele.get_text()[-8:-4] < yearTwo):\n",
    "                                    print(ele.get_text()[-8:-4])\n",
    "#                                     if(ele.get_text()[-10:-8] <= quaterTwo):\n",
    "#                                         print(ele.get_text()[-10:-8])\n",
    "                                        \n",
    "                                        tempUrl = baseUrl+\"Data/\"+ele.get('href')                         \n",
    "                                        b =time.time()\n",
    "                                        downloadUrl=s.post(tempUrl) ## return type = Response\n",
    "                                        e=time.time()\n",
    "                                        print(tempUrl + \" took \"+ str(e-b)+\" sec\")\n",
    "                                        with ZipFile(BytesIO(downloadUrl.content)) as zfile:\n",
    "                                              zfile.extractall(os.path.join(fileDir, 'adsDataRepo/'+'Historical_data_'+ele.get_text()[-8:-4]+'/'))\n",
    "                                              print(\"File \"+ ele.get_text()+\" Downloaded\")\n",
    "                                        \n",
    "                            while(ele.get_text()[-8:-4] == yearTwo):\n",
    "                                if(ele.get_text()[-10:-8] <= quaterTwo):\n",
    "                                    tempUrl = baseUrl+\"Data/\"+ele.get('href')                         \n",
    "                                    b =time.time()\n",
    "                                    downloadUrl=s.post(tempUrl) ## return type = Response\n",
    "                                    e=time.time()\n",
    "                                    print(tempUrl + \" took \"+ str(e-b)+\" sec\")\n",
    "                                    with ZipFile(BytesIO(downloadUrl.content)) as zfile:\n",
    "                                             zfile.extractall(os.path.join(fileDir, 'adsDataRepo/'+'Historical_data_'+ele.get_text()[-8:-4]+'/'))\n",
    "                                             print(\"File \"+ ele.get_text()+\" Downloaded\")\n",
    "                                \n",
    "                                \n",
    "#                                         if(quarter>quaterTwo and year > yearTwo):\n",
    "#                                             print(\"\")\n",
    "#                                             year=99\n",
    "#                                             quarter=99\n",
    "#                                         if(quarter==4):\n",
    "#                                             year=int(year)+1\n",
    "#                                             quarter=1\n",
    "#                                         quarter=int(quarter)+1\n",
    "                                        \n",
    "                                        \n",
    "                            \n",
    "#                             if(ele.get_text()[-8:-4] == yearTwo):\n",
    "# #                                     print(ele.get_text()[-8:-4])\n",
    "#                                     if(ele.get_text()[-10:-8] == quaterTwo):\n",
    "# #                                         print(ele.get_text()[-10:-8])\n",
    "                                        \n",
    "#                                         tempUrl = baseUrl+\"Data/\"+ele.get('href')                         \n",
    "#                                         b =time.time()\n",
    "#                                         downloadUrl=s.post(tempUrl) ## return type = Response\n",
    "#                                         e=time.time()\n",
    "#                                         print(tempUrl + \" took \"+ str(e-b)+\" sec\")\n",
    "#                                         with ZipFile(BytesIO(downloadUrl.content)) as zfile:\n",
    "#                                               zfile.extractall(os.path.join(fileDir, 'adsDataRepo/'+'Historical_data_'+ele.get_text()[-8:-4]+'/'))\n",
    "#                                               print(\"File \"+ ele.get_text()+\" Downloaded\")\n",
    "            else:\n",
    "                print(\"Alert: Please Check the rerouting action suffix\")\n",
    "        \n",
    "        ##To scrape the data from the Site finalUrl.       \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Data  Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def performanceDatacleaning(dataf):\n",
    "#     print(len(dataf))\n",
    "#     df1=dataf.isnull().sum().reset_index()\n",
    "#     df1.columns = ['column_name', 'missing_count']\n",
    "#     df1 = df1.loc[df1['missing_count']>0]\n",
    "#     c=df1.sort_values(by='missing_count',ascending=False)\n",
    "#     c['missing_count']=c['missing_count']/len(dataf)*100\n",
    "#     print(c)\n",
    "\n",
    "#   If null Forward Fill or Backward Fill -  MONTHLY_REPORTING_PERIOD, as its usually apprears in sequence \n",
    "    print('_________________________________________________________')\n",
    "    print('Performance Data Cleaning Started .......')\n",
    "    dataf['MonthlyReportingPeriod'].fillna(method='ffill',inplace=True)\n",
    "    dataf['MonthlyReportingPeriod'].fillna(method='bfill',inplace=True)\n",
    "    dataf['MiRecoveries'].fillna(0,inplace=True)\n",
    "    dataf['NonMiRecoveries'].fillna(0,inplace=True)\n",
    "    dataf['ActualLossCalculation'].fillna(0,inplace=True)\n",
    "    dataf['DueDateOfLastPaidInstallment'].fillna('NA',inplace=True)\n",
    "    dataf['ZeroBalanceCode'].fillna(-1,inplace=True)\n",
    "    dataf['ZeroBalanceEffectiveDate'].fillna('NA',inplace=True)\n",
    "    dataf['RepurchaseFlag'].fillna('NA',inplace=True)\n",
    "    dataf['Modification Cost'].fillna(0,inplace=True)\n",
    "    dataf['MiscellaneousExpenses'].fillna(0,inplace=True)\n",
    "    dataf['TaxesAndInsurance'].fillna(0,inplace=True)\n",
    "    dataf['MaintenanceAndPreservationCosts'].fillna(0,inplace=True)\n",
    "    dataf['LegalCosts'].fillna(0,inplace=True)\n",
    "    dataf['Expenses'].fillna(0,inplace=True)\n",
    "    \n",
    "    dataf['ModificationFlag'].fillna('N',inplace=True)\n",
    "    \n",
    "    dataf['NetSalesProceeds'].fillna('U',inplace=True)\n",
    "    \n",
    "    ## Interpolation of few columns where the missing values are present \n",
    "    dataf['LoanAge']=dataf['LoanAge'].interpolate()\n",
    "    dataf['RemainingMonthsToLegalMaturity']=dataf['RemainingMonthsToLegalMaturity'].interpolate()\n",
    "    dataf['CurrentInterestRate']=dataf['CurrentInterestRate'].interpolate()\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataf['CurrentLoadDelinquencyStatus'].replace('R',-1,inplace=True)\n",
    "    dataf['CurrentLoadDelinquencyStatus'].replace('XX',-2,inplace=True)\n",
    "    dataf['CurrentLoadDelinquencyStatus'].replace(\"   \",0,inplace=True)\n",
    "    dataf['CurrentLoadDelinquencyStatus'].replace('',0,inplace=True)\n",
    "    dataf.CurrentLoadDelinquencyStatus=dataf.CurrentLoadDelinquencyStatus.astype(int)\n",
    "    \n",
    "    print('_____________________________________________________________')\n",
    "    print(\"Data Cleaning of the Performance file Summary  :\")\n",
    "   \n",
    "#     print(\"Data Cleaning Original file info :\")\n",
    "#     df1=dataf.isnull().sum().reset_index()\n",
    "#     df1.columns = ['column_name', 'missing_count']\n",
    "#     df1 = df1.loc[df1['missing_count']>0]\n",
    "#     c=df1.sort_values(by='missing_count',ascending=False)\n",
    "#     c['missing_count%']=c['missing_count']/len(dataf)*100\n",
    "#     print(c)\n",
    "\n",
    "    print(\"Total Null Values Present in Column LoanSequenceNumber \"+ str(dataf['LoanSequenceNumber'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column MonthlyReportingPeriod \"+ str(dataf['MonthlyReportingPeriod'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column CurrentActualUpb \"+ str(dataf['CurrentActualUpb'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column CurrentLoadDelinquencyStatus \"+ str(dataf['CurrentLoadDelinquencyStatus'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column LoanAge \"+ str(dataf['LoanAge'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column RemainingMonthsToLegalMaturity \"+ str(dataf['RemainingMonthsToLegalMaturity'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column RepurchaseFlag \"+ str(dataf['RepurchaseFlag'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column ModificationFlag \"+ str(dataf['ModificationFlag'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column ZeroBalanceCode \"+ str(dataf['ZeroBalanceCode'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column ZeroBalanceEffectiveDate \"+ str(dataf['ZeroBalanceEffectiveDate'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column CurrentInterestRate \"+ str(dataf['CurrentInterestRate'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column CurrentDeferredUpb \"+ str(dataf['CurrentDeferredUpb'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column DueDateOfLastPaidInstallment \"+ str(dataf['DueDateOfLastPaidInstallment'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column MiRecoveries \"+ str(dataf['MiRecoveries'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column NetSalesProceeds \"+ str(dataf['NetSalesProceeds'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column NonMiRecoveries \"+ str(dataf['NonMiRecoveries'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column Expenses \"+ str(dataf['Expenses'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column LegalCosts \"+ str(dataf['LegalCosts'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column MaintenanceAndPreservationCosts \"+ str(dataf['MaintenanceAndPreservationCosts'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column TaxesAndInsurance \"+ str(dataf['TaxesAndInsurance'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column MiscellaneousExpenses \"+ str(dataf['MiscellaneousExpenses'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column ActualLossCalculation \"+ str(dataf['ActualLossCalculation'].isnull().sum()))\n",
    "    print(\"Total Null Values Present in Column Modification Cost \"+ str(dataf['Modification Cost'].isnull().sum()))\n",
    "    print('_____________________________________________________________')\n",
    "    \n",
    "    \n",
    "    return dataf\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data PreProcessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preProcessData(inputQuater,inputYear,inputQuaterTwo,inputYearTwo,fileDir):\n",
    "    print('_________________________________________________________')\n",
    "    print(\"Pre-processing the data........\")\n",
    "    if(os.path.exists(fileDir+'/adsDataRepo/')):\n",
    "#         trainingDataFile = glob.glob(fileDir+'/adsDataRepo/'+'Historical_data_'+inputYear+'/historical_data1_'+inputQuater+inputYear+'.txt')\n",
    "#         testingDataFile = glob.glob(fileDir+'/adsDataRepo/'+'Historical_data_'+inputYearTwo+'/historical_data1_'+inputQuaterTwo+inputYearTwo+'.txt')\n",
    "        trainingPerformanceDataFile = glob.glob(fileDir+'/adsDataRepo/'+'Historical_data_'+inputYear+'/historical_data1_time_'+inputQuater+inputYear+'.txt')\n",
    "        print('Imported Dataset1 ')\n",
    "        testPerformanceDataFile = glob.glob(fileDir+'/adsDataRepo/'+'Historical_data_'+inputYearTwo+'/historical_data1_time_'+inputQuaterTwo+inputYearTwo+'.txt')\n",
    "        print('Imported Dataset2 ')\n",
    "\n",
    "        print(len(trainingPerformanceDataFile))\n",
    "#         headerNames = ['CreditScore','FirstPaymentDate','FirstTimeHomeBuyerFlag','MaturityDate','MSA','MIP','NumberOfUnits',\n",
    "#                          'OccupancyStatus','OCLTV','DTI','OriginalUPB','OLTV','OriginalInterestRate','Channel','PrepaymentPenaltyFlag',\n",
    "#                          'ProductType','PropertyState','PropertyType','PostalCode','LoanSequenceNumber','LoanPurpose',\n",
    "#                          'OriginalLoanTerm','NumberOfBorrowers','SellerName','ServicerName','SuperConformingFlag']\n",
    "        headerNames2 = ['LoanSequenceNumber','MonthlyReportingPeriod','CurrentActualUpb','CurrentLoadDelinquencyStatus',\n",
    "                            'LoanAge','RemainingMonthsToLegalMaturity','RepurchaseFlag','ModificationFlag','ZeroBalanceCode',\n",
    "                            'ZeroBalanceEffectiveDate','CurrentInterestRate','CurrentDeferredUpb','DueDateOfLastPaidInstallment',\n",
    "                            'MiRecoveries','NetSalesProceeds','NonMiRecoveries','Expenses','LegalCosts',\n",
    "                            'MaintenanceAndPreservationCosts','TaxesAndInsurance','MiscellaneousExpenses','ActualLossCalculation',\n",
    "                            'Modification Cost']\n",
    "        \n",
    "#         with open(trainingDataFile[0]) as f:\n",
    "#             dataf = pd.read_table(f, sep='|', low_memory=False, header=None,lineterminator='\\n', names= headerNames)\n",
    "#             cleandataOne = originationDatacleaning(dataf)\n",
    "#             cleandataOne.to_csv(fileDir+'/adsDataRepo/Historical_data_'+inputYear+\"/Origination_Classification_\"+inputQuater+inputYear+\".csv\",index=False)\n",
    "#             print(\"training original data cleaned, CSV Created\")\n",
    "       \n",
    "#         with open(testingDataFile[0]) as f:\n",
    "#             dataf = pd.read_table(f, sep='|', low_memory=False, header=None,lineterminator='\\n', names= headerNames)\n",
    "#             cleandataTwo = originationDatacleaning(dataf)\n",
    "#             cleandataTwo.to_csv(fileDir+'/adsDataRepo/Historical_data_'+inputYear+\"/Origination_Classification_\"+inputQuaterTwo+inputYearTwo+\".csv\",index=False)\n",
    "#             print(\"testing original data cleaned, CSV Created\")\n",
    "            \n",
    "        with open(trainingPerformanceDataFile[0]) as f:\n",
    "            dataf2 = pd.read_table(f, sep='|', low_memory=False,header=None,lineterminator='\\n',names= headerNames2,\n",
    "                                     dtype={'ZeroBalanceCode':str, 'CurrentLoadDelinquencyStatus':str, \n",
    "                                                 'ModificationFlag':str,'NetSalesProceeds':str, 'LegalCosts':str, \n",
    "                                                 'MaintenanceAndPreservationCosts':str, 'TaxesAndInsurance':str, \n",
    "                                                 'Expenses':str, 'MiscellaneousExpenses':str })\n",
    "            cleanperfTrain = performanceDatacleaning(dataf2)\n",
    "            cleanperfTrain.to_csv(fileDir+'/adsDataRepo/Historical_data_'+inputYear+\"/Performance_Classification_\"+inputQuater+inputYear+\".csv\",index=False)\n",
    "            print(\"training performance data cleaned, CSV Created\")\n",
    "            \n",
    "        with open(testPerformanceDataFile[0]) as f:\n",
    "            dataft2 = pd.read_table(f, sep='|', low_memory=False,header=None,lineterminator='\\n',names= headerNames2,\n",
    "                                     dtype={'ZeroBalanceCode':str, 'CurrentLoadDelinquencyStatus':str, \n",
    "                                                 'ModificationFlag':str,'NetSalesProceeds':str, 'LegalCosts':str, \n",
    "                                                 'MaintenanceAndPreservationCosts':str, 'TaxesAndInsurance':str, \n",
    "                                                 'Expenses':str, 'MiscellaneousExpenses':str })\n",
    "            cleanperfTest = performanceDatacleaning(dataft2)\n",
    "            cleanperfTest.to_csv(fileDir+'/adsDataRepo/Historical_data_'+inputYear+\"/Performance_Classification_\"+inputQuaterTwo+inputYearTwo+\".csv\",index=False)\n",
    "            print(\"testing performance data cleaned, CSV Created\")\n",
    "            \n",
    "        return cleanperfTrain,cleanperfTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Main Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "        \n",
    "    print('_________________________________________________________')\n",
    "    print(\"Inside MainFunction .....................\")\n",
    "    print('_________________________________________________________')\n",
    "    fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    print(\"Current Working Directory::::\")\n",
    "    print(fileDir)\n",
    "    baseUrl='https://freddiemac.embs.com/FLoan/'\n",
    "    postUrl='Data/download.php'\n",
    "    username= raw_input('Please enter your Username')\n",
    "    password= raw_input('Please enter your Password')\n",
    "    creds=createCredentialData(username,password)\n",
    "#     creds=createCredentialData(\"maiti.t@husky.neu.edu\",\"3J\\G\\{K4\")\n",
    "    baseUrl='https://freddiemac.embs.com/FLoan/'\n",
    "    postUrl='Data/download.php'\n",
    "    inputQuater='Q1'\n",
    "    inputyear='2000'\n",
    "    inputTestYear='2000'\n",
    "    ## Making the Directory , if not exits before \n",
    "    if(not os.path.exists(fileDir+'/adsDataRepo/')):\n",
    "        os.makedirs(fileDir+'/adsDataRepo/')\n",
    "    inputQuater= raw_input('Enter Quarter ')\n",
    "    inputYear= raw_input('Enter year ')\n",
    "    count=0\n",
    "#     print(inputQuater[1:2])\n",
    "    if(len(inputQuater)!=2 and len(inputYear)!=4):\n",
    "        print(\"Please Enter Valid Input Format \")\n",
    "        inputQuater= raw_input('Enter Start Quarter: ')\n",
    "        inputYear= raw_input('Enter Startyear: ')\n",
    "        inputQuater2= raw_input('Enter End Quarter: ')\n",
    "        inputYear2= raw_input('Enter End year: ')\n",
    "        count+=1\n",
    "        \n",
    "\n",
    "    \n",
    "    try:\n",
    "#             print(type(int(inputYear)))\n",
    "        if(int(inputQuater[1:2])<=4 and int(inputQuater[1:2]) >=1 and int(inputYear) <= 2016 and int(inputYear) >=1999):\n",
    "             if(int(inputQuater[1:2])<=4 and int(inputQuater[1:2]) >=1 and int(inputYear) <= 2016 and int(inputYear) >=1999):\n",
    "        \n",
    "\n",
    "#             temp = int(inputQuater[1:2])\n",
    "#             if(temp==4):\n",
    "#                 temp=1\n",
    "#                 inputTestYear=str(int(inputYear)+1)\n",
    "#             else:\n",
    "#                 temp +=1\n",
    "#                 inputTestYear=str(int(inputYear))\n",
    "#             t=str(temp)\n",
    "\n",
    "                    inputTestQuater=0\n",
    "                    inputTestYear=0\n",
    "                   \n",
    "                \n",
    "#                     jobs = []\n",
    "#                     for ele in links_list:\n",
    "#                         p = multiprocessing.Process(target=downloadFiles(ele, logger, fileDir, httpSession))\n",
    "#                         jobs.append(p)\n",
    "#                         p.start()\n",
    "                    \n",
    "\n",
    "\n",
    "                    getFilesFromFreddieMac(creds,inputQuater,inputYear,inputQuater2,inputYear2,baseUrl,postUrl,fileDir)\n",
    "                    y1=inputYear\n",
    "                    q1=inputQuater\n",
    "                    y2=1999\n",
    "                    q2=1\n",
    "            \n",
    "                    while(y1<inputYear2):\n",
    "                            if(q1==4):\n",
    "                                y2=int(y1)+1\n",
    "                                q2=str(1)\n",
    "                            else:\n",
    "                                q2=str(int(q1)+1)\n",
    "                            \n",
    "                            \n",
    "                            cleanperfTrain,cleanperfTest=preProcessData(q1,y1,q2,y2,fileDir)\n",
    "                            datainsight(cleanperfTrain,cleanperfTest,q2,y2)\n",
    "                            \n",
    "                            if(q1==4):\n",
    "                                y1=str(int(y1)+1)\n",
    "                                q1=str(int(q1)+1)\n",
    "                   \n",
    "                    if(y1==inputYear2):\n",
    "                            while(q1<=inputQuater2):\n",
    "                                q2=str(int(q1)+1)\n",
    "                                y2=str(y1)\n",
    "                                cleanperfTrain,cleanperfTest=preProcessData(q1,y1,q2,y2,fileDir)\n",
    "                                datainsight(cleanperfTrain,cleanperfTest,q2,y2)\n",
    "                    \n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Alert: Please Enter Valid Quater Time : < = 4 and input Year 1999-2016 \")\n",
    "#             year = int(inputYear)+1\n",
    "            inputQuater= raw_input('Enter Quarter')\n",
    "            inputYear= raw_input('Enter year')\n",
    "            inputQuater2= raw_input('Enter End Quarter: ')\n",
    "            inputYear2= raw_input('Enter End year: ')\n",
    "            count+=1\n",
    "\n",
    "        if(count ==5):\n",
    "            print(\"Please restart the Script.\")\n",
    "            count=0\n",
    "    except:\n",
    "        print(\"Please Retry with Valid Input Format \")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Unique Value Present in the Delinquency Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def uniqueValesDelinquency(cleanperfTrain):\n",
    "    print('_________________________________________________________')\n",
    "    print('Unique Values Present in the Training Dataset ')\n",
    "    print('_________________________________________________________')\n",
    "    print(cleanperfTrain.CurrentLoadDelinquencyStatus.unique())\n",
    "    print('_________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transforming the Delinquency Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transformDelinqColumn(cleanperfTrain,cleanperfTest):\n",
    "    print('_________________________________________________________')\n",
    "    print(\"Tranforming Delinquency Columns .......\")\n",
    "    print('_________________________________________________________')\n",
    "    cleanperfTrain['DelinquencyStatus'] = cleanperfTrain.CurrentLoadDelinquencyStatus.map(lambda x: 1 if x > 0 else 0 )\n",
    "    cleanperfTest['DelinquencyStatus'] = cleanperfTest.CurrentLoadDelinquencyStatus.map(lambda x: 1 if x > 0 else 0 )\n",
    "    \n",
    "#     print(type(cleanperfTrain['DelinquencyStatus']))\n",
    "    return cleanperfTrain,cleanperfTest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Factorize Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def factorizeCategoricalColumn(cleanperfTrain,cleanperfTest):\n",
    "        print('_________________________________________________________')\n",
    "        print('Factorizing the Categorical Columns .....................')\n",
    "        print('_________________________________________________________')\n",
    "\n",
    "        cleanperfTrain['RepurchaseFlag_Fact'] = pd.factorize(cleanperfTrain['RepurchaseFlag'])[0]\n",
    "        cleanperfTrain['ModificationFlag_Fact'] = pd.factorize(cleanperfTrain['ModificationFlag'])[0]\n",
    "        cleanperfTrain['ZeroBalanceCode_Fact'] = pd.factorize(cleanperfTrain['ZeroBalanceCode'])[0]\n",
    "        cleanperfTrain['NetSalesProceeds_Fact'] = pd.factorize(cleanperfTrain['NetSalesProceeds'])[0]\n",
    "        cleanperfTest['RepurchaseFlag_Fact'] = pd.factorize(cleanperfTest['RepurchaseFlag'])[0]\n",
    "        cleanperfTest['ModificationFlag_Fact'] = pd.factorize(cleanperfTest['ModificationFlag'])[0]\n",
    "        cleanperfTest['ZeroBalanceCode_Fact'] = pd.factorize(cleanperfTest['ZeroBalanceCode'])[0]\n",
    "        cleanperfTest['NetSalesProceeds_Fact'] = pd.factorize(cleanperfTest['NetSalesProceeds'])[0]\n",
    "        \n",
    "        return cleanperfTrain,cleanperfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train = pd.DataFrame()\n",
    "# x_train = cleanperfTrain[['CurrentActualUpb','LoanAge','RemainingMonthsToLegalMaturity','CurrentInterestRate',\n",
    "#                           'CurrentDeferredUpb','MiRecoveries','NonMiRecoveries','Expenses','LegalCosts',\n",
    "#                             'MaintenanceAndPreservationCosts','TaxesAndInsurance',\n",
    "#                             'MiscellaneousExpenses','ActualLossCalculation',\n",
    "#                             'Modification Cost','RepurchaseFlag_Fact','ModificationFlag_Fact',\n",
    "#                           'ZeroBalanceCode_Fact','NetSalesProceeds_Fact']]\n",
    "# y_train = pd.DataFrame()\n",
    "# y_train['traindelinq'] = cleanperfTrain['DelinquencyStatus']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Recursive feature elimination with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.\n",
    "</p>\n",
    "<p> AS we can see after Count number of 5 features the Cross Validation Score Decreases Drastically.  </p>\n",
    "<b> Note </b><p> We have taken all the columns to analyse for us in this method </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def featureSelection(cleanperfTrain,cleanperfTest):\n",
    "    print('_________________________________________________________')\n",
    "    print('Feature Selection Started ...')\n",
    "    print('_________________________________________________________')\n",
    "    print('Currenlty Commented Out RFEDV Method  as it takes >5 hours ')\n",
    "#     rfecv(cleanperfTrain)\n",
    "\n",
    "    x_train = pd.DataFrame()\n",
    "    x_train = cleanperfTrain[['CurrentActualUpb','LoanAge','RemainingMonthsToLegalMaturity','CurrentInterestRate',\n",
    "                          'CurrentDeferredUpb','MiRecoveries','NonMiRecoveries','Expenses','LegalCosts',\n",
    "                            'MaintenanceAndPreservationCosts','TaxesAndInsurance',\n",
    "                            'MiscellaneousExpenses','ActualLossCalculation',\n",
    "                            'Modification Cost','RepurchaseFlag_Fact','ModificationFlag_Fact',\n",
    "                          'ZeroBalanceCode_Fact','NetSalesProceeds_Fact']]\n",
    "    y_train = pd.DataFrame()\n",
    "    y_train['traindelinq'] = cleanperfTrain['DelinquencyStatus']\n",
    "    ranks = rfe(x_train,y_train)\n",
    "    \n",
    "    trainData,testData,temp=finalfeatureSelection(ranks,cleanperfTrain,cleanperfTest)\n",
    "    \n",
    "    logitnalysis(cleanperfTrain,trainData)\n",
    "    \n",
    "    return temp,trainData,testData\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# svc = SVC(kernel=\"linear\")\n",
    "# # The \"accuracy\" scoring is proportional to the number of correct\n",
    "# # classifications\n",
    "# rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "#               scoring='accuracy')\n",
    "# rfecv.fit(x, y)\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rfecv(cleanperfTrain):\n",
    "    print('_________________________________________________________')\n",
    "    print('Starting Recursive feature elimination with cross-validation')\n",
    "    print('_________________________________________________________')\n",
    "    print('It may take a take time to do the analysis ')\n",
    "    b =time.time()\n",
    "    x_train = pd.DataFrame()\n",
    "    sample=cleanperfTrain.sample(100)\n",
    "    x_train = sample[['CurrentActualUpb','LoanAge','RemainingMonthsToLegalMaturity','CurrentInterestRate',\n",
    "                          'CurrentDeferredUpb','MiRecoveries','NonMiRecoveries','Expenses','LegalCosts',\n",
    "                            'MaintenanceAndPreservationCosts','TaxesAndInsurance',\n",
    "                            'MiscellaneousExpenses','ActualLossCalculation',\n",
    "                            'Modification Cost','RepurchaseFlag_Fact','ModificationFlag_Fact',\n",
    "                          'ZeroBalanceCode_Fact','NetSalesProceeds_Fact']]\n",
    "    y_train = pd.DataFrame()\n",
    "    y_train['traindelinq'] = cleanperfTrain['DelinquencyStatus']\n",
    "    \n",
    "    y=y_train.head(100)\n",
    "    y = np.ravel(y)  \n",
    "    x = np.asarray(x_train.head(100))\n",
    "    \n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    # The \"accuracy\" scoring is proportional to the number of correct\n",
    "    # classifications\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),scoring='accuracy')\n",
    "    print(type(x))\n",
    "    print('gggg')\n",
    "    print(y)\n",
    "    rfecv.fit(x,y)\n",
    "\n",
    "    e=time.time()\n",
    "    print('Total Time Taken : '+str(e-b)+ \"sec\")\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    \n",
    "    rfecv.ranking_\n",
    "    \n",
    "    ranking_out = zip(map(lambda x: round(x, 8), rfecv.ranking_), x_train.columns)\n",
    "    \n",
    "    print(sorted(ranking_out))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal number of features : 5  Note: Run Time : 4 hours Approximately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for the best Top 5 Features:\n",
    "<p>Current Interest Rate </p>\n",
    "<p> Loan Age </p>\n",
    "<p> Remaining Months TO Legal Maturity </p>\n",
    "<p> Repurchase Flag </p>\n",
    "<p>Zero Balance Code </p>\n",
    "<p>Current Actual UPb </p>\n",
    "<b> Note </b><p> Loan Age and Remaining Months TO Legal Maturity Columns represent the same notion in the data </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recursive feature elimination example showing the relevance of pixels in a digit classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rfe(x_train,y_train):\n",
    "    print('_________________________________________________________')\n",
    "    print('Starting Recursive Feature Elimination ....................')\n",
    "    print('_________________________________________________________')\n",
    "    b=time.time()\n",
    "    lr = LogisticRegression()\n",
    "    y_train2=np.ravel(y_train)   \n",
    "    rfe = RFE(estimator=lr, n_features_to_select=5, step=2)\n",
    "    rfe.fit(x_train.head(1000000),y_train2[0:1000000])\n",
    "    ranking = rfe.ranking_\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "    rfe.ranking_\n",
    "    ranks = zip(map(lambda x: round(x, 4), rfe.ranking_), x_train.columns)\n",
    "    ranks = sorted(ranks)\n",
    "    print(\"Result :::\")\n",
    "    for v in ranks:\n",
    "        print(v)\n",
    "    return ranks\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finalfeatureSelection(ranks,cleanperfTrain,cleanperfTest):\n",
    "    print('_________________________________________________________')\n",
    "    print('Final Feature Selection Process Started ................')\n",
    "    print('_________________________________________________________')\n",
    "    temp=[]\n",
    "    for v in ranks:\n",
    "        if(v[0]==1.0):\n",
    "            temp.append(v[1])\n",
    "    temp.append('CurrentActualUpb') ## got from Recursive feature elimination with cross-validation\n",
    "    trainData = cleanperfTrain[temp]\n",
    "    testData = cleanperfTest[temp]\n",
    "    return trainData,testData,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create the RFE object and rank each pixel\n",
    "# svc = SVC(kernel=\"linear\", C=1)\n",
    "# rfe1 = RFE(estimator=svc, n_features_to_select=1, step=2)\n",
    "# rfe1.fit(x_train.head(100),y_train2[0:100])\n",
    "# # ranking = rfe.ranking_.reshape(digits.images[0].shape)\n",
    "\n",
    "# # Plot pixel ranking\n",
    "# plt.matshow(ranking, cmap=plt.cm.Blues)\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Ranking of pixels with RFE\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.Logit Function - Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have not taken in account of the below Columns : \n",
    "<p>As we know Actual Loss = (Default UPB – Net Sale_Proceeds) + DelinquentAccrued Interest \n",
    "                                                              - Expenses – MI Recoveries – Non MIRecoveries</p>\n",
    "<p> And Delinquent Accrued Interest = (Default_Upb – Non Interestbearing UPB)* \n",
    "                                                            (Current Interest rate – 0.35) *\n",
    "                                                            ( Months between Last Principal & \n",
    "                                                              Interest paid to date and zero balance date ) *30/360/100</p>\n",
    "<p>So We have all the DAta information related to Loss that are included in the Actual Loss Calculation Column. SO we are won't be considering the below Columns:</p>\n",
    "    \n",
    "<p>Expenses = Sum(Legal Costs, Maintenance and Preservation Costs, Taxes and                                                      Insurance,Miscellaneous Expenses)</p>\n",
    "<p>    NetSalesProceeds</p>\n",
    "<p>    MiRecoveries</p>\n",
    " <p>   NonMiRecoveries</p>\n",
    " <p>   Legal Costs</p>\n",
    " <p>   Maintenance and Preservation Costs</p>\n",
    " <p>   Taxes and Insurance</p>\n",
    "<p>   MiscellaneousExpenses\n",
    "    </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logit - Function of  Logs of Odds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logitnalysis(cleanperfTrain,trainData):\n",
    "        print('_________________________________________________________')\n",
    "        print(\"Starting Logit Analysis ............\")\n",
    "        print('_________________________________________________________')\n",
    "        b=time.time()\n",
    "        logit = sm.Logit(np.asarray(cleanperfTrain['DelinquencyStatus'].head(1000000)),np.asarray(trainData.head(1000000)))\n",
    "        logitodd = logit.fit()\n",
    "        print(logitodd.summary2())\n",
    "        e=time.time()\n",
    "        print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "        print(\"The estimated coefficients are the log odds. By exponentiating these values, we can calculate the odds, which are easier to interpret.\")\n",
    "        print('_________________________________________________________')\n",
    "        print(np.exp(logitodd.params))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The estimated coefficients are the log odds. By exponentiating these values, we can calculate the odds, which are easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result : Below are are the features whose Standard Error are CLose to Zero : \n",
    "<p> CurrentActualUpb</p>\n",
    "<p> LoanAge</p>\n",
    "<p> RemainingMonthsToLegalMaturity</p>\n",
    "<p> ActualLossCalculation</p>\n",
    "<p>Modification Cost</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are doing regression on Dependent Variable Binary(Delinquency Status ) with the below features :\n",
    "<p>CurrentInterestRate</p>\n",
    "<p>LoanAge</p>\n",
    "<p>NetSalesProceeds_Fact</p>\n",
    "<p>RepurchaseFlag_Fact</p>\n",
    "<p>ZeroBalanceCode_Fact</p>\n",
    "<p>CurrentActualUpb</p>\n",
    "<p>ActualLossCalculation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def regression(temp,trainData,testData,cleanperfTrain,cleanperfTest,q2,y2):\n",
    "        print('_________________________________________________________')\n",
    "        print(\"Starting Classification .............\")\n",
    "        print('_________________________________________________________')\n",
    "        y_train2=np.ravel(cleanperfTrain['DelinquencyStatus'])\n",
    "        deliq_test=np.ravel(cleanperfTest['DelinquencyStatus'])\n",
    "        datainfo=float(np.unique(y_train2,return_counts=True)[1][1])/float(len(y_train2))*100\n",
    "        print(\"Percentage of Delinquency Status Flag present in the Test Dataset \"+\n",
    "              str(datainfo)+str(\"%\"))\n",
    "        tempCA=[]\n",
    "        tempActDelinq=[]\n",
    "        tempPredDelinq=[]\n",
    "        tempTotalRec=[]\n",
    "        tempDelinqProp=[]\n",
    "        tempNonDelinq=[]\n",
    "        \n",
    "        reportVar=[tempCA,tempActDelinq,tempPredDelinq,tempTotalRec,tempDelinqProp,tempNonDelinq]\n",
    "        if(datainfo<25):\n",
    "            print(\"Caution: Dataset is Imbalanced:::Predictive Modelling may not be efficient for the Delinqency Status Flag\")\n",
    "            bestScore=balancedDataClassification(cleanperfTrain,cleanperfTest,temp,reportVar,q2,y2)\n",
    "            print(\"Original Dataset Clssification\")\n",
    "            bestScore=originalDataClassification(trainData,testData,cleanperfTrain,cleanperfTest,reportVar,q2,y2)\n",
    "            \n",
    "        else:\n",
    "            print(\"Original Dataset Clssification\")\n",
    "            bestScore=originalDataClassification(trainData,testData,cleanperfTrain,cleanperfTest,reportVar,q2,y2)\n",
    "        \n",
    "        return bestScore\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Balancing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def balancedDataClassification(cleanperfTrain,cleanperfTest,temp,reportVar,q2,y2):\n",
    "        print('_________________________________________________________')\n",
    "        print(\"Balancing the data\")\n",
    "        print('_________________________________________________________')\n",
    "        databalancetemp=temp\n",
    "        databalancetemp.append('DelinquencyStatus')\n",
    "        y_train2=np.ravel(cleanperfTrain['DelinquencyStatus'])\n",
    "        trainBalanceData = cleanperfTrain[databalancetemp]\n",
    "        trainBalanceData1=trainBalanceData.loc[trainBalanceData['DelinquencyStatus'] == 0]\n",
    "        trainBalanceData1=trainBalanceData1.sample(trainBalanceData['DelinquencyStatus'].sum()*3)\n",
    "        \n",
    "        trainBalanceData2=trainBalanceData.loc[trainBalanceData['DelinquencyStatus'] == 1]\n",
    "        frame1=[trainBalanceData1,trainBalanceData2]\n",
    "        a=len(trainBalanceData1)\n",
    "        b=len(trainBalanceData2)\n",
    "        \n",
    "        print(\"Current Dataset insight\")\n",
    "        print(\"Non Delinquent present Percentage\"+str(a/(a+b)*100))\n",
    "        print(\"Delinquent present Percentage\"+str(b/(a+b)*100))\n",
    "        finaltrainBalaneData=pd.concat(frame1)\n",
    "        x_trainData=finaltrainBalaneData[temp]\n",
    "        y_trainData=np.ravel(finaltrainBalaneData['DelinquencyStatus'])\n",
    "\n",
    "        testBalanceData = cleanperfTest[databalancetemp]\n",
    "        testBalanceData1=testBalanceData.loc[testBalanceData['DelinquencyStatus'] == 0]\n",
    "        testBalanceData1=testBalanceData1.sample(testBalanceData['DelinquencyStatus'].sum())\n",
    "        testBalanceData2=testBalanceData.loc[testBalanceData['DelinquencyStatus'] == 1]\n",
    "        frame2=[testBalanceData1,testBalanceData2]\n",
    "        finaltestBalaneData=pd.concat(frame2)\n",
    "        x_testData=finaltestBalaneData[temp]\n",
    "        y_testData=np.ravel(finaltestBalaneData['DelinquencyStatus'])\n",
    "        \n",
    "        print(\"##################################################\")\n",
    "        print('Balanced DataSet Classification')\n",
    "        tempScore=[]\n",
    "        tempScore,reportVar=LogisticRegressionClassification(x_trainData,y_trainData,x_testData,y_testData,reportVar,tempScore)\n",
    "        tempScore,reportVar=randomForesClassifier(x_trainData,y_trainData,x_testData,y_testData,reportVar,tempScore)\n",
    "        tempScore,reportVar=neuralClssification(x_trainData,y_trainData,x_testData,y_testData,reportVar,tempScore)\n",
    "        bestScore=evaluateBestModel(tempScore)\n",
    "        \n",
    "        createReport(reportVar,q2,y2)\n",
    "        \n",
    "        reportVar=[]\n",
    "        tempScore=[]\n",
    "        return bestScore\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Original Data Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def originalDataClassification(trainData,testData,cleanperfTrain,cleanperfTest,reportVar,q2,y2):\n",
    "    print(\"Starting Original  Data Classification\")\n",
    "    print('_________________________________________________________')\n",
    "    y_train2=np.ravel(cleanperfTrain['DelinquencyStatus'])\n",
    "    deliq_test=np.ravel(cleanperfTest['DelinquencyStatus'])\n",
    "    tempScore=[]\n",
    "    tempScore,reportVar=LogisticRegressionClassification(trainData,y_train2,testData,deliq_test,reportVar,tempScore)\n",
    "    tempScore,reportVar=randomForesClassifier(trainData,y_train2,testData,deliq_test,reportVar,tempScore)\n",
    "    tempScore,reportVar=neuralClssification(trainData,y_train2,testData,deliq_test,reportVar,tempScore)\n",
    "    \n",
    "    bestScore = evaluateBestModel(tempScore)\n",
    "    createReport(reportVar,q2,y2)\n",
    "    tempScore=[]\n",
    "    reportVar=[]\n",
    "    return bestScore\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LogisticRegressionClassification(trainData,y_train2,testData,deliq_test,reportVar,tempScore):\n",
    "        b=time.time()\n",
    "        print('Starting Logistic Regession Classification')\n",
    "        print('_________________________________________________________')\n",
    "        modellr = LogisticRegression()\n",
    "        modellr.fit(trainData,y_train2)\n",
    "        e=time.time()\n",
    "        print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "        #Check the accuracy of the model\n",
    "        print(\"Accuracy of the logistic Regression Classification Model Training Dataset: \"+ str(modellr.score(trainData,y_train2)))\n",
    "        \n",
    "        ##Prediction og regression\n",
    "#         tempScore=[]\n",
    "        b=time.time()\n",
    "        resultpred = modellr.predict(testData)\n",
    "        probs = modellr.predict_proba(testData)[:, 1]\n",
    "        # generate evaluation metrics\n",
    "        scoreLogisitic = metrics.accuracy_score(deliq_test, resultpred)\n",
    "        print('Accuracy of the logistic Regression Classification Model Test Dataset: ' + str(scoreLogisitic))\n",
    "        e=time.time()\n",
    "        print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "        \n",
    "        \n",
    "        ## Generating Confusion Matrix :::::::::::::::\n",
    "        b=time.time()\n",
    "        cm = metrics.confusion_matrix(deliq_test, resultpred)\n",
    "        print('Confusion Matrix:')\n",
    "        print(cm)\n",
    "        e=time.time()\n",
    "        print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "        \n",
    "        p = cm[0][0]\n",
    "        t = np.unique(deliq_test,return_counts=True)[1][0]\n",
    "        r=float(p)/float(t)*100\n",
    "        print(\"Percentage of Predicting right Non - Delinquency Status Flag   \" +str(r)+str(\"%\"))\n",
    "        d=cm[1][1]\n",
    "        t=np.unique(deliq_test,return_counts=True)[1][1]\n",
    "        r=float(d)/float(t)*100\n",
    "        print(\"Percentage of Predicting right Delinquency Status Flag   \" +str(r)+str(\"%\"))\n",
    "        tempScore.append(r)\n",
    "        \n",
    "        ## Vizualisation of the Confusion Matrix \n",
    "        fix, ax = plt.subplots(figsize=(8, 6))\n",
    "        plt.suptitle('Confusion Matrix  on Data Set')\n",
    "        matrix = cm\n",
    "        plt.title('Logistic Regression Classification on Test Data');\n",
    "        sns.heatmap(matrix, annot=True,  fmt='');\n",
    "        \n",
    "        deliq_test = deliq_test.astype(np.float)\n",
    "        fpr, tpr, _ = metrics.roc_curve(deliq_test, probs)\n",
    "        ## Plotting ROC Curve ------------\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title('ROC curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        reportVar[0].append('Logistic Regression') ##Classification_Algorithm\n",
    "        reportVar[1].append(np.unique(deliq_test,return_counts=True)[1][1]) ## Number_of_Actual_Delinquents\n",
    "        reportVar[2].append(cm[0][1]+cm[1][1])##Number_of_predicted_delinquents\n",
    "        reportVar[3].append(len(deliq_test))##Number_of_records_in_dataset\n",
    "        reportVar[4].append(cm[1][1])##Number_of_delinquents_properly_classified\n",
    "        reportVar[5].append(cm[1][0])##Number_of_non_delinquents_improperly_classified_as_delinquents\n",
    "        \n",
    "        return tempScore,reportVar\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute confusion matrix to evaluate the accuracy of a classification\n",
    "#### By definition a confusion matrix C is such that C_{i, j} is equal to the \n",
    "#### number of observations known to be in group i but predicted to be in group j.\n",
    "#### Thus in binary classification, the count of true negatives is C_{0,0}, false negatives is C_{1,0}, true positives is C_{1,1} and false positives is C_{0,1}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def randomForesClassifier(trainData,y_train2,testData,deliq_test,reportVar,tempScore):\n",
    "    print('_________________________________________________________')\n",
    "    print('Random Forest Classification Started ...........')\n",
    "    print('_________________________________________________________')\n",
    "    b=time.time()\n",
    "    model = RandomForestClassifier(n_jobs=2)\n",
    "    model = model.fit(trainData,y_train2)\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "    \n",
    "    print(\"Accuracy of the Random Forest Classification Model on Training Dataset : \"+ str(model.score(trainData,y_train2)))\n",
    "    ##Prediction \n",
    "    b=time.time()\n",
    "    resultpred = model.predict(testData)\n",
    "    probs = model.predict_proba(testData)[:, 1]\n",
    "    # generate evaluation metrics\n",
    "    scorerf = metrics.accuracy_score(deliq_test, resultpred)\n",
    "    print('\"Accuracy of the Random Forest Classification Model on Test Dataset:' + str(scorerf))\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "#     tempScore.append(scorerf)\n",
    "\n",
    "    ## Creating Confusion Matrix \n",
    "    b=time.time()\n",
    "    cm = metrics.confusion_matrix(deliq_test, resultpred)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "    \n",
    "    ## visualizing the confusion Matrix \n",
    "    fix, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.suptitle('Confusion Matrix  on Data Set')\n",
    "    matrix = cm\n",
    "    plt.title('Random Forest on Test Data');\n",
    "    sns.heatmap(matrix, annot=True,  fmt='');\n",
    "    plt.show()\n",
    "    \n",
    "    ##ROC \n",
    "    deliq_test = deliq_test.astype(np.float)\n",
    "    fpr, tpr, _ = metrics.roc_curve(deliq_test, probs)\n",
    "    #Plot ROC curve\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "    p = cm[0][0]\n",
    "    t = np.unique(deliq_test,return_counts=True)[1][0]\n",
    "    r=float(p)/float(t)*100\n",
    "    print(\"Percentage of Predicting right Non - Delinquency Status Flag   \" +str(r)+str(\"%\"))\n",
    "    d=cm[1][1]\n",
    "    t=np.unique(deliq_test,return_counts=True)[1][1]\n",
    "    r=float(d)/float(t)*100\n",
    "    print(\"Percentage of Predicting right Delinquency Status Flag   \" +str(r)+str(\"%\"))\n",
    "    tempScore.append(r)\n",
    "\n",
    "    reportVar[0].append('Random Forest Classification') ##Classification_Algorithm\n",
    "    reportVar[1].append(np.unique(deliq_test,return_counts=True)[1][1]) ## Number_of_Actual_Delinquents\n",
    "    reportVar[2].append(cm[0][1]+cm[1][1])##Number_of_predicted_delinquents\n",
    "    reportVar[3].append(len(deliq_test))##Number_of_records_in_dataset\n",
    "    reportVar[4].append(cm[1][1])##Number_of_delinquents_properly_classified\n",
    "    reportVar[5].append(cm[1][0])##Number_of_non_delinquents_improperly_classified_as_delinquents\n",
    "    \n",
    "    return tempScore,reportVar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Neural Network Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def neuralClssification(trainData,y_train2,testData,deliq_test,reportVar,tempScore):\n",
    "    print('_________________________________________________________')\n",
    "    print('Neural Network Classfication Started ')\n",
    "    print('_________________________________________________________')\n",
    "    b=time.time()\n",
    "    model = neural_network.MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    model = model.fit(trainData, y_train2)\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "    print(\"Accuracy of the Neural Network Classification Model on Training Dataset : \"+ str(model.score(trainData,y_train2)))\n",
    "    b=time.time()\n",
    "    result = model.predict(testData)\n",
    "    probs = model.predict_proba(testData)[:, 1]\n",
    "    # generate evaluation metrics\n",
    "    scorenn = metrics.accuracy_score(deliq_test, result)\n",
    "    print('\"Accuracy of the  Neural Network Classification Model on Test Dataset :' + str(scorenn))\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "#     tempScore.append(scorenn)\n",
    "\n",
    "    b=time.time()\n",
    "    cm = metrics.confusion_matrix(deliq_test, result)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    e=time.time()\n",
    "    print(\"Time in Processing : \"+str(e-b)+\"sec\")\n",
    "\n",
    "    \n",
    "    fix, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.suptitle('Confusion Matrix  on Data Set')\n",
    "    matrix = cm\n",
    "    plt.title('Neural Network on Test Data');\n",
    "    sns.heatmap(matrix, annot=True,  fmt='');\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    deliq_test = deliq_test.astype(np.float)\n",
    "    fpr, tpr, _ = metrics.roc_curve(deliq_test, probs)\n",
    "\n",
    "\n",
    "    #Plot ROC curve\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    p = cm[0][0]\n",
    "    t = np.unique(deliq_test,return_counts=True)[1][0]\n",
    "    r=float(p)/float(t)*100\n",
    "    print(\"Percentage of Predicting right Non - Delinquency Status Flag   \" +str(r)+str(\"%\"))\n",
    "    d=cm[1][1]\n",
    "    t=np.unique(deliq_test,return_counts=True)[1][1]\n",
    "    r=float(d)/float(t)*100\n",
    "    print(\"Percentage of Predicting right Delinquency Status Flag   \" +str(r)+str(\"%\"))\n",
    "    tempScore.append(r)\n",
    "\n",
    "    \n",
    "    reportVar[0].append('Neural Network Classification') ##Classification_Algorithm\n",
    "    reportVar[1].append(np.unique(deliq_test,return_counts=True)[1][1]) ## Number_of_Actual_Delinquents\n",
    "    reportVar[2].append(cm[0][1]+cm[1][1])##Number_of_predicted_delinquents\n",
    "    reportVar[3].append(len(deliq_test))##Number_of_records_in_dataset\n",
    "    reportVar[4].append(cm[1][1])##Number_of_delinquents_properly_classified\n",
    "    reportVar[5].append(cm[1][0])##Number_of_non_delinquents_improperly_classified_as_delinquents\n",
    "    \n",
    "    return tempScore,reportVar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Evaluating the Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluateBestModel(tempScore):\n",
    "        print('_________________________________________________________')\n",
    "        print('Calulating Best Model ...')\n",
    "        print('_________________________________________________________')\n",
    "        bestScore=pd.DataFrame(tempScore)\n",
    "        bestScore.columns=['Score']\n",
    "        bestScore.index=['LogisticRegression','RandomForest','NeuralNetWork']\n",
    "        bestScore=bestScore.sort_values(by='Score',ascending =False)\n",
    "        print(\"Best Classification Algorithm : \"+str(bestScore.index[0])+\"   Accuracy \"+str(bestScore.Score[0]))\n",
    "        print(\"Result in Nutshell : \")\n",
    "        print(bestScore)\n",
    "        return bestScore\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.Creating report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createReport(reportVar,q2,y2):\n",
    "        print('_________________________________________________________')\n",
    "        print('Creating Report .....')\n",
    "        print('_________________________________________________________')\n",
    "        bestClassification = pd.DataFrame()\n",
    "        bestClassification['Classification_Algorithm']=reportVar[0]\n",
    "        bestClassification['Number_of_Actual_Delinquents']=reportVar[1]\n",
    "        bestClassification['Number_of_predicted_delinquents']=reportVar[2]\n",
    "        bestClassification['Number_of_records_in_dataset']=reportVar[3]\n",
    "        bestClassification['Number_of_delinquents_properly_classified']=reportVar[4]\n",
    "        bestClassification['Number_of_non_delinquents_improperly_classified_as_delinquents']=reportVar[5]\n",
    "        print(bestClassification)\n",
    "        pd.to_csv(\"Report\"+str(q2)+str(y1))\n",
    "        print('_________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Data Insight Function Calling all the defined functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datainsight(cleanperfTrain,cleanperfTest,q2,y2):\n",
    "    \n",
    "    print('_________________________________________________________')\n",
    "    print(\"Shape of Training Data :\"+ str(cleanperfTrain.shape))\n",
    "    print('_________________________________________________________')\n",
    "    print(\"Description of Training Data\")\n",
    "    print(cleanperfTrain.describe())\n",
    "\n",
    "    uniqueValesDelinquency(cleanperfTrain)\n",
    "\n",
    "    cleanperfTrain,cleanperfTest=transformDelinqColumn(cleanperfTrain,cleanperfTest)\n",
    "\n",
    "    print(cleanperfTrain.DelinquencyStatus.unique())\n",
    "    print('_________________________________________________________')\n",
    "    print(\"Total Training Delinquency Flag\"+str(cleanperfTrain.DelinquencyStatus.sum()))\n",
    "    print('_________________________________________________________')\n",
    "    print(\"Total TEst Delinquency Flag \"+str(cleanperfTest.DelinquencyStatus.sum()))\n",
    "\n",
    "    cleanperfTrain,cleanperfTest=factorizeCategoricalColumn(cleanperfTrain,cleanperfTest)\n",
    "\n",
    "    # print(\"Columns  \")\n",
    "    # print(cleanperfTrain.columns)\n",
    "\n",
    "    temp,trainData,testData =featureSelection(cleanperfTrain,cleanperfTest)\n",
    "\n",
    "\n",
    "    bestScore =regression(temp,trainData,testData,cleanperfTrain,cleanperfTest,q2,y2)\n",
    "    return bestScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. The Driver Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________\n",
      "Inside MainFunction .....................\n",
      "_________________________________________________________\n",
      "Current Working Directory::::\n",
      "/home/Tridiv\n",
      "Please enter your Usernamemaiti.t@husky.neu.edu\n",
      "Please enter your Password|:h3DUry\n",
      "_________________________________________________________\n",
      "Creating Credentials ....\n",
      "Enter Quarter Q4\n",
      "Enter year 2008\n",
      "_________________________________________________________\n",
      "GettingFile from Freddie Mac ......\n",
      "Step1: Logged in\n",
      "Step2 : Terms and Conditions Accepted\n",
      "Step3: Filtered the Sample Files with Condition =2008\n",
      "Status::::::::::\n",
      "https://freddiemac.embs.com/FLoan/Data/download.php?f=historical_data1_Q42008&s=88338222 took 10.5129988194 sec\n",
      "File historical_data1_Q42008.zip Downloaded\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    cleanperfTrain,cleanperfTest = main()  \n",
    "#     bestScore=datainsight(cleanperfTrain,cleanperfTest)\n",
    "#     inputuser= raw_input('Do want to Remodel Again with newer Dataset ')\n",
    "#     alltrain=pd.DataFrame()\n",
    "#     alltrain=pd.concat([cleanperfTrain,cleanperfTest])\n",
    "#     if \"Yes\" or \"Y\" or \"y\" in inputuser:\n",
    "#         cleanperfTrain1,cleanperfTest2 = main() \n",
    "#         alltrain=pd.concat([alltrain,cleanperfTrain1])\n",
    "#         alltest=cleanperfTest2\n",
    "#         bestScore1=datainsight(alltrain,alltest)\n",
    "#         print(bestScore1)\n",
    "#         print(bestScore)\n",
    "#         inputuser= raw_input('Do want to Remodel Again with newer Dataset,which helps our Model to improve its accuracy')\n",
    "#     elif \"No\" or \"n\" or \"N\" in inputuser:\n",
    "#         print(\"Data Classification finished.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
